{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "class Corpus(object):\n",
    "    def __init__(self, path='./data'):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = os.path.join(path, 'train.txt')\n",
    "        self.test = os.path.join(path, 'test.txt')\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    "    \n",
    "# RNN Based Language Model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x) \n",
    "        \n",
    "        # Forward propagate RNN  \n",
    "        out, h = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time step\n",
    "        out = self.linear(out)  \n",
    "        return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000   # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load Penn Treebank Dataset\n",
    "train_path = './data/train.txt'\n",
    "sample_path = './sample.txt'\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data(train_path, batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [Variable(state.data) for state in states] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1549], Loss: 9.212, Perplexity: 10012.28\n",
      "Epoch [1/5], Step[100/1549], Loss: 6.026, Perplexity: 414.23\n",
      "Epoch [1/5], Step[200/1549], Loss: 5.932, Perplexity: 376.74\n",
      "Epoch [1/5], Step[300/1549], Loss: 5.804, Perplexity: 331.55\n",
      "Epoch [1/5], Step[400/1549], Loss: 5.696, Perplexity: 297.77\n",
      "Epoch [1/5], Step[500/1549], Loss: 5.142, Perplexity: 171.08\n",
      "Epoch [1/5], Step[600/1549], Loss: 5.206, Perplexity: 182.31\n",
      "Epoch [1/5], Step[700/1549], Loss: 5.402, Perplexity: 221.81\n",
      "Epoch [1/5], Step[800/1549], Loss: 5.212, Perplexity: 183.39\n",
      "Epoch [1/5], Step[900/1549], Loss: 5.108, Perplexity: 165.26\n",
      "Epoch [1/5], Step[1000/1549], Loss: 5.165, Perplexity: 174.96\n",
      "Epoch [1/5], Step[1100/1549], Loss: 5.335, Perplexity: 207.46\n",
      "Epoch [1/5], Step[1200/1549], Loss: 5.166, Perplexity: 175.21\n",
      "Epoch [1/5], Step[1300/1549], Loss: 5.094, Perplexity: 163.09\n",
      "Epoch [1/5], Step[1400/1549], Loss: 4.825, Perplexity: 124.56\n",
      "Epoch [1/5], Step[1500/1549], Loss: 5.181, Perplexity: 177.91\n",
      "Epoch [2/5], Step[0/1549], Loss: 5.499, Perplexity: 244.40\n",
      "Epoch [2/5], Step[100/1549], Loss: 4.677, Perplexity: 107.47\n",
      "Epoch [2/5], Step[200/1549], Loss: 4.745, Perplexity: 115.01\n",
      "Epoch [2/5], Step[300/1549], Loss: 4.867, Perplexity: 129.97\n",
      "Epoch [2/5], Step[400/1549], Loss: 4.733, Perplexity: 113.58\n",
      "Epoch [2/5], Step[500/1549], Loss: 4.242, Perplexity: 69.52\n",
      "Epoch [2/5], Step[600/1549], Loss: 4.556, Perplexity: 95.23\n",
      "Epoch [2/5], Step[700/1549], Loss: 4.592, Perplexity: 98.67\n",
      "Epoch [2/5], Step[800/1549], Loss: 4.538, Perplexity: 93.53\n",
      "Epoch [2/5], Step[900/1549], Loss: 4.439, Perplexity: 84.68\n",
      "Epoch [2/5], Step[1000/1549], Loss: 4.473, Perplexity: 87.60\n",
      "Epoch [2/5], Step[1100/1549], Loss: 4.731, Perplexity: 113.41\n",
      "Epoch [2/5], Step[1200/1549], Loss: 4.598, Perplexity: 99.27\n",
      "Epoch [2/5], Step[1300/1549], Loss: 4.428, Perplexity: 83.73\n",
      "Epoch [2/5], Step[1400/1549], Loss: 4.120, Perplexity: 61.56\n",
      "Epoch [2/5], Step[1500/1549], Loss: 4.476, Perplexity: 87.88\n",
      "Epoch [3/5], Step[0/1549], Loss: 4.807, Perplexity: 122.42\n",
      "Epoch [3/5], Step[100/1549], Loss: 4.076, Perplexity: 58.88\n",
      "Epoch [3/5], Step[200/1549], Loss: 4.187, Perplexity: 65.81\n",
      "Epoch [3/5], Step[300/1549], Loss: 4.196, Perplexity: 66.41\n",
      "Epoch [3/5], Step[400/1549], Loss: 4.096, Perplexity: 60.11\n",
      "Epoch [3/5], Step[500/1549], Loss: 3.688, Perplexity: 39.97\n",
      "Epoch [3/5], Step[600/1549], Loss: 4.085, Perplexity: 59.44\n",
      "Epoch [3/5], Step[700/1549], Loss: 4.015, Perplexity: 55.40\n",
      "Epoch [3/5], Step[800/1549], Loss: 4.024, Perplexity: 55.91\n",
      "Epoch [3/5], Step[900/1549], Loss: 3.834, Perplexity: 46.22\n",
      "Epoch [3/5], Step[1000/1549], Loss: 3.944, Perplexity: 51.63\n",
      "Epoch [3/5], Step[1100/1549], Loss: 4.185, Perplexity: 65.72\n",
      "Epoch [3/5], Step[1200/1549], Loss: 4.083, Perplexity: 59.30\n",
      "Epoch [3/5], Step[1300/1549], Loss: 3.829, Perplexity: 46.02\n",
      "Epoch [3/5], Step[1400/1549], Loss: 3.555, Perplexity: 35.00\n",
      "Epoch [3/5], Step[1500/1549], Loss: 3.865, Perplexity: 47.69\n",
      "Epoch [4/5], Step[0/1549], Loss: 4.122, Perplexity: 61.70\n",
      "Epoch [4/5], Step[100/1549], Loss: 3.587, Perplexity: 36.12\n",
      "Epoch [4/5], Step[200/1549], Loss: 3.753, Perplexity: 42.65\n",
      "Epoch [4/5], Step[300/1549], Loss: 3.748, Perplexity: 42.42\n",
      "Epoch [4/5], Step[400/1549], Loss: 3.622, Perplexity: 37.40\n",
      "Epoch [4/5], Step[500/1549], Loss: 3.247, Perplexity: 25.71\n",
      "Epoch [4/5], Step[600/1549], Loss: 3.701, Perplexity: 40.49\n",
      "Epoch [4/5], Step[700/1549], Loss: 3.567, Perplexity: 35.40\n",
      "Epoch [4/5], Step[800/1549], Loss: 3.623, Perplexity: 37.44\n",
      "Epoch [4/5], Step[900/1549], Loss: 3.309, Perplexity: 27.36\n",
      "Epoch [4/5], Step[1000/1549], Loss: 3.443, Perplexity: 31.28\n",
      "Epoch [4/5], Step[1100/1549], Loss: 3.693, Perplexity: 40.16\n",
      "Epoch [4/5], Step[1200/1549], Loss: 3.550, Perplexity: 34.82\n",
      "Epoch [4/5], Step[1300/1549], Loss: 3.357, Perplexity: 28.71\n",
      "Epoch [4/5], Step[1400/1549], Loss: 3.034, Perplexity: 20.78\n",
      "Epoch [4/5], Step[1500/1549], Loss: 3.437, Perplexity: 31.10\n",
      "Epoch [5/5], Step[0/1549], Loss: 3.606, Perplexity: 36.83\n",
      "Epoch [5/5], Step[100/1549], Loss: 3.218, Perplexity: 24.97\n",
      "Epoch [5/5], Step[200/1549], Loss: 3.296, Perplexity: 27.00\n",
      "Epoch [5/5], Step[300/1549], Loss: 3.320, Perplexity: 27.67\n",
      "Epoch [5/5], Step[400/1549], Loss: 3.336, Perplexity: 28.11\n",
      "Epoch [5/5], Step[500/1549], Loss: 2.929, Perplexity: 18.70\n",
      "Epoch [5/5], Step[600/1549], Loss: 3.399, Perplexity: 29.93\n",
      "Epoch [5/5], Step[700/1549], Loss: 3.214, Perplexity: 24.87\n",
      "Epoch [5/5], Step[800/1549], Loss: 3.265, Perplexity: 26.18\n",
      "Epoch [5/5], Step[900/1549], Loss: 3.012, Perplexity: 20.33\n",
      "Epoch [5/5], Step[1000/1549], Loss: 3.139, Perplexity: 23.09\n",
      "Epoch [5/5], Step[1100/1549], Loss: 3.315, Perplexity: 27.51\n",
      "Epoch [5/5], Step[1200/1549], Loss: 3.285, Perplexity: 26.71\n",
      "Epoch [5/5], Step[1300/1549], Loss: 3.069, Perplexity: 21.51\n",
      "Epoch [5/5], Step[1400/1549], Loss: 2.705, Perplexity: 14.96\n",
      "Epoch [5/5], Step[1500/1549], Loss: 3.118, Perplexity: 22.60\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    # Initial hidden and memory states\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get batch inputs and targets\n",
    "        inputs = Variable(ids[:, i:i+seq_length])\n",
    "        targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        model.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states) \n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                   (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version <unk> who jones chairman dennis <unk> r. utah in a position in N after he was a <unk> \n",
      "most former chairmen had <unk> been called <unk> for N most years ago at the agency and its <unk> texas energy operation \n",
      "so far crunch can close to de new job and also <unk> their own wives \n",
      "it can even maintain its music above it \n",
      "<unk> is the crown prince and <unk> operation supermarket & <unk> \n",
      "common <unk> prosecutors featured a $ <unk> a <unk> <unk> <unk> to a design when a treatment made a stronger debate toward <unk> <unk> in N \n",
      "for everyone frightened it was to leave it clear even \n",
      "i would n't find who readers are going into all scenes going off or even never yet to be abc or <unk> to baltimore \n",
      "eventually they <unk> he says \n",
      "though it continues to convey that schedules into the basic system still <unk> protein to post which were one of many <unk> students expectations and federal <unk> salaries after the third game after the time \n",
      "the league is to draw someone again when you will buy \n",
      "or eight season left the winner \n",
      "besides resources the <unk> has held a loss of $ N or almost tell cbs news \n",
      "meanwhile as in N it seemed to be will share the scenes without certain this for the ninth to $ N a share \n",
      "the chairman real estate of the sponsors said <unk> by a <unk> <unk> institution became a <unk> of employee business magazine <unk> <unk> a <unk> mass. <unk> ranges from a boring <unk> and N sports <unk> at a <unk> news conference call apples their productions \n",
      "ad was n't fully cut mr. jones 's successor says \n",
      "these days <unk> <unk> consider a <unk> note that was on an shop magazine \n",
      "the novel is the most four key to mr. roman is a black appreciation \n",
      "mr. wood says and his latest has a commercial bank on the project \n",
      "with the heart news ogilvy that <unk> the navy broadcast with a <unk> post that makes <unk> in texas and a tough east germany which is several cbs networks \n",
      "mr. <unk> 's ogilvy could succeed arthur l. \n",
      "jerry brown \n",
      "keeping the giants giants spend $ N million were on mr. <unk> <unk> a <unk> salesman & <unk> walking into the better than a year 's N <unk> \n",
      "<unk> <unk> was named to an lotus suitable telephone group mr. <unk> takes control \n",
      "a combined with a bank to settle the newly hired customers as chief executive \n",
      "ogilvy & mather has a top of them in N but was in such ventures as a blow and a rival of italian airlines a producer for the houston box firm of chicago-based connaught <unk> \n",
      "in the case sony will pay $ N in each case on an <unk> flow to about N executives of <unk> of california \n",
      "by contrast they have a hard way and that even less commercial and provide our presence the <unk> with what has to be made \n",
      "i 'm going to be selling for college until N N to manage quite voluntarily \n",
      "factory stadium has finally gotten active in minnesota \n",
      "this is another garden <unk> <unk> rocks stories about bonn 's nbc news stories about N rooms advertising \n",
      "they 're N N asbestos per in south and that from naturally great \n",
      "some former growers notably the mouse chapter may took the <unk> season \n",
      "the league is in <unk> that she was assistant of the baseball act on old <unk> and high scientists <unk> <unk> and a beer magazine <unk> \n",
      "mr. <unk> was a <unk> copies from several other three questions at the wine now on the <unk> projection \n",
      "<unk> interviewed afternoon and <unk> journalism went back to their <unk> \n",
      "but while most one <unk> off with a television news the first members of a new series of today took a city effort to communist european news at the <unk> museum troop to N people marched to N N the last season with the virus \n",
      "he 's recalls the team <unk> baseball altogether publicity became missing in his institute as collateral were n't just walking out \n",
      "although the manager made a nearly <unk> interesting party he 'd night \n",
      "he will definitely remember the new red \n",
      "ms. chung 's withdrawal is the nfl time \n",
      "i 'm not all man life we 'll be moved at another time their house says no surprise \n",
      "<unk> makes it bobby <unk> walking me as as a television definition \n",
      "instead posting it how rep. filters sells the <unk> for a <unk> post \n",
      "in the suit to its mother <unk> her husband <unk> johnson 's <unk> proposed series N affair abc news news for a <unk> owners contest about design sports camp actors off the sweat \n",
      "at a weather this magazine he <unk> first phillips to a <unk> court camp at cbs news around dean football fifth and <unk> both as bobby thomson was texas as the tire master \n",
      "and in the league he pledged to seem <unk> by news news news wires at the game manager president de <unk> <unk> news reports saying he were present \n",
      "<unk> m. jerry a <unk> student <unk> <unk> the <unk> dream of the longer and bobby thomson was a <unk> company well-known mr. <unk> at the university of puerto rico to fill his team struck \n",
      "N years old after it absorbed \n",
      "along with <unk> fiber when it was easy to <unk> that \n",
      "only after a giant exhibition the <unk> national jailed <unk> a huge blue <unk> in the hole immediately for <unk> the 1980s \n",
      "the <unk> black market <unk> me to express \n",
      "just cbs started into second the american express job mr. \n"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "with open(sample_path, 'w') as f:\n",
    "    # Set intial hidden ane memory states\n",
    "    state = (Variable(torch.zeros(num_layers, 1, hidden_size)),\n",
    "         Variable(torch.zeros(num_layers, 1, hidden_size)))\n",
    "\n",
    "    # Select one word id randomly\n",
    "    prob = torch.ones(vocab_size)\n",
    "    input = Variable(torch.multinomial(prob, num_samples=1).unsqueeze(1),\n",
    "                     volatile=True)\n",
    "    s = \"\"\n",
    "    for i in range(num_samples):\n",
    "        # Forward propagate rnn \n",
    "        output, state = model(input, state)\n",
    "        \n",
    "        # Sample a word id\n",
    "        prob = output.squeeze().data.exp()\n",
    "        word_id = torch.multinomial(prob, 1)[0]\n",
    "        \n",
    "        # Feed sampled word id to next time step\n",
    "        input.data.fill_(word_id)\n",
    "        \n",
    "        # File write\n",
    "        word = corpus.dictionary.idx2word[word_id]\n",
    "        word = '\\n' if word == '<eos>' else word + ' '\n",
    "        s += word\n",
    "    print (s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
