{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "class Corpus(object):\n",
    "    def __init__(self, path='./data'):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = os.path.join(path, 'train.txt')\n",
    "        self.test = os.path.join(path, 'test.txt')\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    "    \n",
    "# RNN Based Language Model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x) \n",
    "        \n",
    "        # Forward propagate RNN  \n",
    "        out, h = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time step\n",
    "        out = self.linear(out)  \n",
    "        return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000   # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load Shakespeare Dataset\n",
    "train_path = './data/shakespeare.txt'\n",
    "sample_path = './sample.txt'\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data(train_path, batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [Variable(state.data) for state in states] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1714], Loss: 11.125, Perplexity: 67864.73\n",
      "Epoch [1/5], Step[100/1714], Loss: 6.953, Perplexity: 1045.93\n",
      "Epoch [1/5], Step[200/1714], Loss: 6.975, Perplexity: 1070.05\n",
      "Epoch [1/5], Step[300/1714], Loss: 6.429, Perplexity: 619.43\n",
      "Epoch [1/5], Step[400/1714], Loss: 6.570, Perplexity: 713.67\n",
      "Epoch [1/5], Step[500/1714], Loss: 6.059, Perplexity: 427.86\n",
      "Epoch [1/5], Step[600/1714], Loss: 6.157, Perplexity: 471.82\n",
      "Epoch [1/5], Step[700/1714], Loss: 6.173, Perplexity: 479.39\n",
      "Epoch [1/5], Step[800/1714], Loss: 6.671, Perplexity: 789.55\n",
      "Epoch [1/5], Step[900/1714], Loss: 6.059, Perplexity: 427.95\n",
      "Epoch [1/5], Step[1000/1714], Loss: 6.040, Perplexity: 419.99\n",
      "Epoch [1/5], Step[1100/1714], Loss: 6.056, Perplexity: 426.59\n",
      "Epoch [1/5], Step[1200/1714], Loss: 5.678, Perplexity: 292.38\n",
      "Epoch [1/5], Step[1300/1714], Loss: 5.352, Perplexity: 210.96\n",
      "Epoch [1/5], Step[1400/1714], Loss: 6.047, Perplexity: 423.02\n",
      "Epoch [1/5], Step[1500/1714], Loss: 5.751, Perplexity: 314.36\n",
      "Epoch [1/5], Step[1600/1714], Loss: 5.501, Perplexity: 244.87\n",
      "Epoch [1/5], Step[1700/1714], Loss: 5.534, Perplexity: 253.23\n",
      "Epoch [2/5], Step[0/1714], Loss: 5.885, Perplexity: 359.54\n",
      "Epoch [2/5], Step[100/1714], Loss: 4.757, Perplexity: 116.44\n",
      "Epoch [2/5], Step[200/1714], Loss: 5.461, Perplexity: 235.27\n",
      "Epoch [2/5], Step[300/1714], Loss: 5.081, Perplexity: 160.90\n",
      "Epoch [2/5], Step[400/1714], Loss: 5.379, Perplexity: 216.86\n",
      "Epoch [2/5], Step[500/1714], Loss: 5.008, Perplexity: 149.53\n",
      "Epoch [2/5], Step[600/1714], Loss: 5.108, Perplexity: 165.35\n",
      "Epoch [2/5], Step[700/1714], Loss: 5.208, Perplexity: 182.68\n",
      "Epoch [2/5], Step[800/1714], Loss: 5.606, Perplexity: 272.07\n",
      "Epoch [2/5], Step[900/1714], Loss: 5.168, Perplexity: 175.59\n",
      "Epoch [2/5], Step[1000/1714], Loss: 5.027, Perplexity: 152.55\n",
      "Epoch [2/5], Step[1100/1714], Loss: 5.054, Perplexity: 156.70\n",
      "Epoch [2/5], Step[1200/1714], Loss: 4.783, Perplexity: 119.44\n",
      "Epoch [2/5], Step[1300/1714], Loss: 4.464, Perplexity: 86.86\n",
      "Epoch [2/5], Step[1400/1714], Loss: 5.207, Perplexity: 182.47\n",
      "Epoch [2/5], Step[1500/1714], Loss: 4.733, Perplexity: 113.61\n",
      "Epoch [2/5], Step[1600/1714], Loss: 4.577, Perplexity: 97.27\n",
      "Epoch [2/5], Step[1700/1714], Loss: 4.638, Perplexity: 103.37\n",
      "Epoch [3/5], Step[0/1714], Loss: 4.902, Perplexity: 134.60\n",
      "Epoch [3/5], Step[100/1714], Loss: 4.033, Perplexity: 56.45\n",
      "Epoch [3/5], Step[200/1714], Loss: 4.539, Perplexity: 93.60\n",
      "Epoch [3/5], Step[300/1714], Loss: 4.340, Perplexity: 76.70\n",
      "Epoch [3/5], Step[400/1714], Loss: 4.564, Perplexity: 95.95\n",
      "Epoch [3/5], Step[500/1714], Loss: 4.184, Perplexity: 65.64\n",
      "Epoch [3/5], Step[600/1714], Loss: 4.399, Perplexity: 81.35\n",
      "Epoch [3/5], Step[700/1714], Loss: 4.312, Perplexity: 74.61\n",
      "Epoch [3/5], Step[800/1714], Loss: 4.524, Perplexity: 92.21\n",
      "Epoch [3/5], Step[900/1714], Loss: 4.402, Perplexity: 81.61\n",
      "Epoch [3/5], Step[1000/1714], Loss: 4.107, Perplexity: 60.74\n",
      "Epoch [3/5], Step[1100/1714], Loss: 4.133, Perplexity: 62.38\n",
      "Epoch [3/5], Step[1200/1714], Loss: 3.921, Perplexity: 50.45\n",
      "Epoch [3/5], Step[1300/1714], Loss: 3.713, Perplexity: 40.96\n",
      "Epoch [3/5], Step[1400/1714], Loss: 4.345, Perplexity: 77.11\n",
      "Epoch [3/5], Step[1500/1714], Loss: 3.765, Perplexity: 43.18\n",
      "Epoch [3/5], Step[1600/1714], Loss: 3.729, Perplexity: 41.64\n",
      "Epoch [3/5], Step[1700/1714], Loss: 3.833, Perplexity: 46.20\n",
      "Epoch [4/5], Step[0/1714], Loss: 3.829, Perplexity: 46.00\n",
      "Epoch [4/5], Step[100/1714], Loss: 3.505, Perplexity: 33.27\n",
      "Epoch [4/5], Step[200/1714], Loss: 3.874, Perplexity: 48.13\n",
      "Epoch [4/5], Step[300/1714], Loss: 3.751, Perplexity: 42.56\n",
      "Epoch [4/5], Step[400/1714], Loss: 3.857, Perplexity: 47.32\n",
      "Epoch [4/5], Step[500/1714], Loss: 3.580, Perplexity: 35.87\n",
      "Epoch [4/5], Step[600/1714], Loss: 3.856, Perplexity: 47.29\n",
      "Epoch [4/5], Step[700/1714], Loss: 3.695, Perplexity: 40.24\n",
      "Epoch [4/5], Step[800/1714], Loss: 3.734, Perplexity: 41.86\n",
      "Epoch [4/5], Step[900/1714], Loss: 3.733, Perplexity: 41.81\n",
      "Epoch [4/5], Step[1000/1714], Loss: 3.512, Perplexity: 33.52\n",
      "Epoch [4/5], Step[1100/1714], Loss: 3.510, Perplexity: 33.44\n",
      "Epoch [4/5], Step[1200/1714], Loss: 3.294, Perplexity: 26.95\n",
      "Epoch [4/5], Step[1300/1714], Loss: 3.071, Perplexity: 21.55\n",
      "Epoch [4/5], Step[1400/1714], Loss: 3.714, Perplexity: 41.02\n",
      "Epoch [4/5], Step[1500/1714], Loss: 3.173, Perplexity: 23.88\n",
      "Epoch [4/5], Step[1600/1714], Loss: 3.197, Perplexity: 24.47\n",
      "Epoch [4/5], Step[1700/1714], Loss: 3.214, Perplexity: 24.87\n",
      "Epoch [5/5], Step[0/1714], Loss: 3.233, Perplexity: 25.37\n",
      "Epoch [5/5], Step[100/1714], Loss: 3.047, Perplexity: 21.05\n",
      "Epoch [5/5], Step[200/1714], Loss: 3.415, Perplexity: 30.41\n",
      "Epoch [5/5], Step[300/1714], Loss: 3.294, Perplexity: 26.95\n",
      "Epoch [5/5], Step[400/1714], Loss: 3.402, Perplexity: 30.04\n",
      "Epoch [5/5], Step[500/1714], Loss: 3.152, Perplexity: 23.39\n",
      "Epoch [5/5], Step[600/1714], Loss: 3.444, Perplexity: 31.30\n",
      "Epoch [5/5], Step[700/1714], Loss: 3.307, Perplexity: 27.30\n",
      "Epoch [5/5], Step[800/1714], Loss: 3.400, Perplexity: 29.97\n",
      "Epoch [5/5], Step[900/1714], Loss: 3.358, Perplexity: 28.73\n",
      "Epoch [5/5], Step[1000/1714], Loss: 3.226, Perplexity: 25.18\n",
      "Epoch [5/5], Step[1100/1714], Loss: 3.089, Perplexity: 21.96\n",
      "Epoch [5/5], Step[1200/1714], Loss: 2.948, Perplexity: 19.07\n",
      "Epoch [5/5], Step[1300/1714], Loss: 2.824, Perplexity: 16.84\n",
      "Epoch [5/5], Step[1400/1714], Loss: 3.274, Perplexity: 26.43\n",
      "Epoch [5/5], Step[1500/1714], Loss: 2.821, Perplexity: 16.79\n",
      "Epoch [5/5], Step[1600/1714], Loss: 2.830, Perplexity: 16.94\n",
      "Epoch [5/5], Step[1700/1714], Loss: 2.859, Perplexity: 17.44\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    # Initial hidden and memory states\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get batch inputs and targets\n",
    "        inputs = Variable(ids[:, i:i+seq_length])\n",
    "        targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        model.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states) \n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                   (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the place. \n",
      "But Our eternal wife, Leave all your arms \n",
      "To dew of wife when I have bank'd and prune \n",
      "The cock, his house with all modest men. \n",
      "GLOUCESTER. Some things, dear gentle wife, unless a goodly man \n",
      "Defy me on a great and crown'd shame? \n",
      "Sir M. Doubt not, my lord. \n",
      "TIMON. Hear that thou diest. \n",
      "MESSENGER. He dares not. \n",
      "CHARMIAN. I know you then. That you confess, \n",
      "In this division of your great times \n",
      "I give him back your thanks, and weeds away. \n",
      "First, welcome to King John and meet me here. \n",
      "CITIZEN. Ha! Will not you lose your Grace's blow! \n",
      "KING HENRY. The reasons of the bad are by your house, \n",
      "Because our mettle is a law so lightly \n",
      "In this same book of Fortune? knife are come \n",
      "And tell how question ceremony. So was our death; \n",
      "Here in the lobby. \n",
      "MALCOLM. What shall this be? \n",
      "MACDUFF. Upon all fortunes, you come hither \n",
      "How honour in our time your purpos'd service \n",
      "In light and feeding, I am Egypt's hand, being rank, \n",
      "Nor many misery, have accus'd a right. \n",
      "Doug. Here is the noble creature of our father, \n",
      "Beheld of many enemies and his hard friend, \n",
      "At once like devils. O, which myself have follow'd \n",
      "To hear some danger voice. it built upon \n",
      "The envious barking of life. \n",
      "A courteous avarice \n",
      "And if it be a story now i' th' war, \n",
      "And all the cranny may admit right, though \n",
      "I am as am sweet country's speaking of him now. \n",
      "Only the difference of vice upon a kingly \n",
      "As his sister, in a voyage, I should hear me \n",
      "To mark how true. \n",
      "AUFIDIUS. 'Tis it too, sir? Come on. \n",
      "MACDUFF. They fight. \n",
      "CHARMIAN. Came you to chide, \n",
      "And, yet no less large than my here-remain \n",
      "That Cassio's a likely servant to him. \n",
      "There is a looking haunts nothing but \n",
      "A dangerous wolf to th' top of such a storm \n",
      "A lioness, would not go about to heaven, \n",
      "As I serv'd him, and he do say that foul \n",
      "As once, to make coals cheap- your good son, \n",
      "But, noble as my albeit We may know ever, \n",
      "'Tis worthy he'd call PLEBEIANS. for once madness \n",
      "Left I upon him there. He craves my sword; \n",
      "Quite like himself. \n",
      "EMILIA. Indeed, sir, Queen- I mean the world? Their husbandry \n",
      "Red ere the complete gods confound \n",
      "To quicken these sad scruples, Barren which they savour \n",
      "That attendant and drawn home proportions \n",
      "To do his honour well, and his kind face \n",
      "Shall that they say he is corrupted liar, \n",
      "He is not then with all most cloak, for life, \n",
      "That cannot cease his idle blood to me, \n",
      "From thence with him; even now I cannot comfort him; \n",
      "And ere thy actions be a discontented man, \n",
      "Or by the dull grace that the moon prove \n",
      "A most miraculous work i' the full world. \n",
      "What says Ulysses? \n",
      "SUFFOLK. Ay. \n",
      "CHARMIAN. Lord Alexas, spake upon us! \n",
      "There on; \n",
      "Only here our main neighbour company? Exit \n",
      "CHARMIAN. At point and armies \n",
      "SOOTHSAYER. I never thought of thee in banishment, \n",
      "A miserable hand. The King and Queen \n",
      "Is Edmund Henry, and the coming on, \n",
      "The tent doth ride upon thee and the Duke. \n",
      "Well, Catesby, if ever thou shalt leave me. \n",
      "O known irons often they do do so, \n",
      "Unless thou sad effect a power worth sorrow. \n",
      "Give me thy York; if the time to weet \n",
      "The scorn of life be short! to patience; \n",
      "Cut him some wine, and bid all fly that bate \n",
      "Show him a pale confusion of yellow, \n",
      "After a huge estate. Once more things else \n",
      "Than it is come with thanks; \n",
      "Dido at once: of heaven shed tears \n",
      "Figur'd quite with all the right. \n",
      "Tongue, not; that rash joy should teach it. \n",
      "SLY. Thus he is so far in a band of Olympus, \n",
      "THERSITES. Wast thou a poet? \n",
      "ROSALIND. never do all my clamours. He was turn'd to a prologue to \n",
      "Menenius sisterhood of them all. \n",
      "ROSALIND. Away, you cullions! hell money well. \n",
      "THERSITES. No. I thank thee, Moon, I rode thee. Give me thy hand, \n",
      "Who finds the world with Apemantus. No. \n",
      "PAGE. Himself and has eaten the window at his liver \n",
      "a pint of meat; ye Fabian, am! is fitting \n",
      "for wolves and hang'd for Hero at http://pglaf.org \n",
      "\n",
      "Enter PATROCLUS \n",
      "\n",
      "PATROCLUS. Who's the tide? \n",
      "Mess. For my faith, \n",
      "Sit down again by lights the English; \n",
      "The other sport of Andren. O loathed gods, \n",
      "Are dearly to his husband, that thy valour \n",
      "Must I be up my by an art a great \n",
      "To Roger law, that thou shalt reap is so; \n",
      "And under Walls shalt thou have prov'd a child, \n",
      "No, shouldst thou call them children, were I guilty? \n",
      "PORTIA. Ten, this kindness, then: it hath been a \n",
      "bad fence ere any of the conqueror; \n",
      "And then found pap, all earth, and point \n",
      "The poor assay that eyeless storm from hell \n",
      "Are burnt in blood, peace, nor part of thine, \n",
      "The King thy heir is rush'd from heavy back. \n",
      "Thus wilt thou leave me in thy unhallowed robes \n",
      "And trust grace to thee. \n",
      "Glou. Hence, and haste and thy thoughts! \n",
      "Good my boy. \n",
      "Exeunt. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scene II. \n",
      "Plain file I in the house of kites \n",
      "Into a Christian Doctor numbers. \n",
      "The \n"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "with open(sample_path, 'w') as f:\n",
    "    # Set intial hidden ane memory states\n",
    "    state = (Variable(torch.zeros(num_layers, 1, hidden_size)),\n",
    "         Variable(torch.zeros(num_layers, 1, hidden_size)))\n",
    "\n",
    "    # Select one word id randomly\n",
    "    prob = torch.ones(vocab_size)\n",
    "    input = Variable(torch.multinomial(prob, num_samples=1).unsqueeze(1),\n",
    "                     volatile=True)\n",
    "    s = \"\"\n",
    "    for i in range(num_samples):\n",
    "        # Forward propagate rnn \n",
    "        output, state = model(input, state)\n",
    "        \n",
    "        # Sample a word id\n",
    "        prob = output.squeeze().data.exp()\n",
    "        word_id = torch.multinomial(prob, 1)[0]\n",
    "        \n",
    "        # Feed sampled word id to next time step\n",
    "        input.data.fill_(word_id)\n",
    "        \n",
    "        # File write\n",
    "        word = corpus.dictionary.idx2word[word_id]\n",
    "        word = '\\n' if word == '<eos>' else word + ' '\n",
    "        s += word\n",
    "    print (s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
